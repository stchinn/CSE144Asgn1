{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE-144-01 - 2025 Winter - HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "- Submit your assignments onto **Canvas** by the due date. Upload a single `.ipynb` file with all the necessary output.\n",
    "- Don't use any magic function from other libraries. You will get **no credit** if a Gradient Descent function from a known library is used when you are asked to implement **Linear Regression** or **Linear Classifcation**.\n",
    "- Don't change the input and output structure of pre-defined functions. Only write your code between `YOUR CODE STARTS HERE` and `YOUR CODE ENDS HERE`. You won't get the points if you alter the code outside these two dividing lines. Most coding parts can be finished with about 5-6 lines of codes.\n",
    "- Make sure you have installed required packages imported in the first code block.\n",
    "\n",
    "## Rubric\n",
    "\n",
    "The assignment is worth 75pts in total:\n",
    "- Data Preprocessing (12pts)\n",
    "    - Remove Missing Values (2pts)\n",
    "    - Remove outliers (5pts)\n",
    "    - Normalization (5pts)\n",
    "- Dataset Preparation (11pts)\n",
    "    - Build TrainVal/Test Set `test_split` (5pts)\n",
    "    - K-fold Cross Validation `train_val_split` (6pts)\n",
    "- Linear Regression (26pts)\n",
    "    - `mean_square_error_loss` (3pts)\n",
    "    - `mse_gradient` (5pts)\n",
    "    - Hyper-Parameters Tuning (6pts)\n",
    "    - Traininig (10pts)\n",
    "        - Translate Dataframe to Numpy (3pts)\n",
    "        - Theta Initialization (1pt)\n",
    "        - Loss Computation (3pts)\n",
    "        - Gradient Descent (3pts)\n",
    "    - Evaluation on test set (2pts)\n",
    "- Linear Classification with Logistic Regression (26pts)\n",
    "    - `sigmoid` (1pt)\n",
    "    - `binary_cross_entrophy_loss` (3pts)\n",
    "    - `bce_gradient` (5pts)\n",
    "    - Hyper-Parameters Tuning (6pts)\n",
    "    - Traininig (9pts)\n",
    "        - `accuracy` (1pt)\n",
    "        - Translate Dataframe to Numpy (2pts)\n",
    "        - Theta Initialization (1pt)\n",
    "        - Loss and ACC Computation (3pts)\n",
    "        - Gradient Descent (2pts)\n",
    "    - Evaluation on test set (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_red = pd.read_csv(\"winequality-red-new.csv\")\n",
    "data_white = pd.read_csv(\"winequality-white-new.csv\")\n",
    "\n",
    "data_red[\"wine_type\"] = 1\n",
    "data_white[\"wine_type\"] = 0\n",
    "\n",
    "data = pd.concat([data_red, data_white], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (12pts)\n",
    "`data.info()` shows that there are some missing values in the dataset. Also, we can see from the histogram that outliers exist for some features. Moreover, the range of different features has a huge gap: from (0, 1), to (0, 300). In the following you need to perform:\n",
    "1. Drop rows that contain NULL values.\n",
    "2. Remove outliers for features in `['chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'residual sugar']` based on interquantile range.\n",
    "3. Normalize of features using Z-score method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values (2pts)\n",
    "Drop rows that have NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== YOUR CODE STARTS HERE ==========\n",
    "# ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers (5pts)\n",
    "Remove outliers for features in `['chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'residual sugar']` based on interquantile range. Here for each feature, we first sort data in an ascending order.\n",
    "\n",
    "Let q1 and q3 be the data that ranks 25% and 75% respectively. We then let `iqr = q3 - q1`, and\n",
    "compute\n",
    "```\n",
    "a = q1 - iqr x 1.5\n",
    "b = q3 + iqr x 1.5\n",
    "```\n",
    "and remove the data out of the range [a, b].\n",
    "\n",
    "Note: this can be realized with function `pd.DataFrame.quantile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    for feature in ['chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'residual sugar']:\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data_cleaned = remove_outliers(data)\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (5pts)\n",
    "Scale all features using z-score normalization.\n",
    "\n",
    "`quality` is the target for linear regression. Scale the `quality` from `[0, 10]` to `[0, 1]` for easier convergence.\n",
    "\n",
    "`wine_type` is the target for linear classification. No normalization is needed for `wine_type` as it is already binary.\n",
    "\n",
    "Note: use different mean and std for different feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== YOUR CODE STARTS HERE ==========\n",
    "# ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "print(\"Mean\\n\", data_normalized.mean(axis=0))\n",
    "print(\"STD\\n\", data_normalized.std(axis=0))\n",
    "\n",
    "assert np.isclose(data_normalized.drop(columns=[\"quality\", \"wine_type\"]).mean(axis=0), 0).all()\n",
    "assert np.isclose(data_normalized.drop(columns=[\"quality\", \"wine_type\"]).std(axis=0), 1).all()\n",
    "assert data_normalized[\"wine_type\"].apply(lambda x: x in [0, 1]).all()\n",
    "assert data_normalized[\"quality\"].apply(lambda x: 0 <= x and x <= 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized.info()\n",
    "data_normalized.hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation (11pts)\n",
    "Next we will split dataset to train set, validation set and test set. First we randomly choose 20% as test set. Then we use $k$-fold validation on the remaining 80% to generate $k$ different train set and validation set pairs, where $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5pts\n",
    "def test_split(data, test_ratio=0.2, seed=SEED):\n",
    "    '''\n",
    "    Use function `train_test_split` to split test set.\n",
    "    '''\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    return (x_train_val.reset_index(drop=True),\n",
    "            y_train_val.reset_index(drop=True),\n",
    "            x_test.reset_index(drop=True),\n",
    "            y_test.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# 6pts\n",
    "def train_val_split(x_train_val, y_train_val, k=5):\n",
    "    '''\n",
    "    Use given index sets to generate k train and validation pairs. The return value should be\n",
    "    a list whose components are tuples:\n",
    "    [(x_train1, y_train1, x_val1, y_val1), ..., (x_traink, y_traink, x_valk, y_valk)]\n",
    "\n",
    "    Each x_traink/y_traink/x_valk/y_valk is a pd.DataFrame\n",
    "\n",
    "    '''\n",
    "    index_shuffle = list(x_train_val.index)\n",
    "    random.shuffle(index_shuffle)\n",
    "\n",
    "    # This is the indices for the validation set for each fold\n",
    "    val_indices_list = [\n",
    "        [\n",
    "            index_shuffle[i + fold_i]\n",
    "            for i in range(0, len(index_shuffle), k) if i + fold_i < len(index_shuffle)\n",
    "        ]\n",
    "        for fold_i in range(k)\n",
    "    ]\n",
    "\n",
    "    train_val_list = []\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "    return train_val_list\n",
    "\n",
    "\n",
    "K = 5\n",
    "x_train_val, y_train_val, x_test, y_test = test_split(data_normalized, test_ratio=0.2)\n",
    "train_val_list = train_val_split(x_train_val, y_train_val, k=K)\n",
    "\n",
    "assert len(train_val_list) == K and {len(fold) for fold in train_val_list} == {4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (26pts)\n",
    "\n",
    "Use linear regression to prediction `quality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3pts\n",
    "def mean_square_error_loss(pred: np.ndarray, target: np.ndarray):\n",
    "    # Normalized by pred.shape[0]\n",
    "\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "# 5pts\n",
    "def mse_gradient(X: np.ndarray, target: np.ndarray, theta: np.ndarray):\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy = np.linspace(1, -1, 64)\n",
    "x_dummy = np.ones((8, 64))\n",
    "pred_dummy = x_dummy @ theta_dummy\n",
    "target_dummy =  np.linspace(1, 0, 8)\n",
    "print(\"Dummy Loss:\", mean_square_error_loss(pred_dummy, target_dummy))\n",
    "print(\"Dummy Gradient Mean & STD:\", mse_gradient(x_dummy, target_dummy, theta_dummy).mean(), mse_gradient(x_dummy, target_dummy, theta_dummy).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters Tuning (6pts)\n",
    "\n",
    "You need to reach an averaged final MSE < 0.05 on 5-fold validation sets to get points for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== YOUR CODE STARTS HERE ==========\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "# ========== YOUR CODE ENDS HERE ============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (10pts)\n",
    "\n",
    "Hint: A convenient way to calculate $X \\cdot \\theta$ is to prepend a column of 1's to your $X$ matrix so that you don't have to perform an extra addition operation for the bias.\n",
    "\n",
    "If you decide to use this implementation, add an additional dimension to your feature matrix so that they are consistent in shape. See the image below for illustration. See this [illustration](https://cs231n.github.io/linear-classify/#:~:text=Illustration%20of%20the,and%20the%20biases.) for more detail.\n",
    "\n",
    "You report should include the train, val and test loss. You can try different learning rates and epoch combinations and report the best results you get based on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_folds = [[] for _ in range(K)]\n",
    "val_losses_folds = [[] for _ in range(K)]\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for i in range(K):\n",
    "    x_train, y_train, x_val, y_val = train_val_list[i]\n",
    "\n",
    "    y_train = y_train[\"quality\"]\n",
    "    y_val = y_val[\"quality\"]\n",
    "\n",
    "    # Translate pd.DataFrame to numpy (3pt)\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    # Initialize theta (1pt)\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "        # Compute loss, and save to train_losses_folds and val_losses_folds (3pts)\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "        # Gradient descent (3pts)\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    # Save trained theta\n",
    "    thetas.append(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Averaged final training loss: {sum([train_losses[-1] for train_losses in train_losses_folds]) / len(train_losses_folds)}\")\n",
    "print(f\"Averaged final validation loss: {sum([val_losses[-1] for val_losses in val_losses_folds]) / len(val_losses_folds)}\")\n",
    "\n",
    "plt.plot(np.arange(num_epochs), train_losses_folds[0], label=\"Train loss\")\n",
    "plt.plot(np.arange(num_epochs), val_losses_folds[0], label=\"Val loss\")\n",
    "plt.title(\"Train + validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set (3pts)\n",
    "\n",
    "Average the predictions from thetas from each fold as the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_final(x_test, thetas):\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "test_pred = inference_final(x_test, thetas)\n",
    "test_loss = mean_square_error_loss(test_pred, y_test[\"quality\"])\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification with Logistic Regression (26pts)\n",
    "\n",
    "Use Logistic Regression to prediction `wine_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1pt\n",
    "def sigmoid(logit):\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "# 3pts\n",
    "def binary_cross_entrophy_loss(pred, target):\n",
    "    # Normalized by pred.shape[0]\n",
    "\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "# 5pts\n",
    "def bce_gradient(X, target, theta):\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy = np.linspace(1, -1, 64)\n",
    "x_dummy = np.ones((8, 64))\n",
    "pred_dummy = sigmoid(x_dummy @ theta_dummy)\n",
    "target_dummy =  np.linspace(1, 0, 8)\n",
    "print(\"Dummy Loss:\", binary_cross_entrophy_loss(pred_dummy, target_dummy))\n",
    "print(\"Dummy Gradient Mean & STD:\", bce_gradient(x_dummy, target_dummy, theta_dummy).mean(), bce_gradient(x_dummy, target_dummy, theta_dummy).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters (6pts)\n",
    "\n",
    "You need to reach an averaged final ACC > 0.85 on 5-fold validation sets to get points for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_threshold = 0.5\n",
    "# ========== YOUR CODE STARTS HERE ==========\n",
    "# ========== YOUR CODE ENDS HERE ============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (9pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1pt\n",
    "def accuracy(pred, target):\n",
    "    # compute accuracy with predicted probability, target, and `prob_threshold` defined in the previous block\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "train_losses_folds = [[] for _ in range(K)]\n",
    "val_losses_folds = [[] for _ in range(K)]\n",
    "\n",
    "train_accs_folds = [[] for _ in range(K)]\n",
    "val_accs_folds = [[] for _ in range(K)]\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for i in range(K):\n",
    "    x_train, y_train, x_val, y_val = train_val_list[i]\n",
    "\n",
    "    y_train = y_train[\"wine_type\"]\n",
    "    y_val = y_val[\"wine_type\"]\n",
    "\n",
    "    # Translate pd.DataFrame to numpy (2pts)\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    # Initialize theta (1pt)\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "        # Compute loss, and save to train_losses_folds and val_losses_folds\n",
    "        # Compute accuracy, and save to train_accs_folds and val_accs_folds\n",
    "        # 3pts\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "        # Gradient descent (2pts)\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "    # Save trained theta\n",
    "    thetas.append(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Averaged final training loss: {sum([train_losses[-1] for train_losses in train_losses_folds]) / len(train_losses_folds)}\")\n",
    "print(f\"Averaged final validation loss: {sum([val_losses[-1] for val_losses in val_losses_folds]) / len(val_losses_folds)}\")\n",
    "print(f\"Averaged final training ACC: {sum([train_accs[-1] for train_accs in train_accs_folds]) / len(train_accs_folds)}\")\n",
    "print(f\"Averaged final validation ACC: {sum([val_accs[-1] for val_accs in val_accs_folds]) / len(val_accs_folds)}\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(np.arange(num_epochs), train_losses_folds[0], label=\"Train loss\")\n",
    "axes[0].plot(np.arange(num_epochs), val_losses_folds[0], label=\"Val loss\")\n",
    "axes[0].set_title(\"Train + Validation Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(np.arange(num_epochs), train_accs_folds[0], label=\"Train ACC\")\n",
    "axes[1].plot(np.arange(num_epochs), val_accs_folds[0], label=\"Val ACC\")\n",
    "axes[1].set_title(\"Train + Validation Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"ACC\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set (2pts)\n",
    "\n",
    "Average the predictions from thetas from each fold as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_final(x_test, thetas):\n",
    "    # ========== YOUR CODE STARTS HERE ==========\n",
    "    # ========== YOUR CODE ENDS HERE ============\n",
    "\n",
    "\n",
    "test_pred = inference_final(x_test, thetas)\n",
    "test_loss = accuracy(test_pred, y_test[\"wine_type\"])\n",
    "print(f\"Test ACC: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
